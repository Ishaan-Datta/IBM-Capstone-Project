{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing wandb.ai workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"IBM Capstone Project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"architecture\": \"RBM-Stacked Autoencoder\",\n",
    "    \"dataset\": \"MovieLens 100K\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "ratings = pd.read_csv('ratings.csv', encoding='utf-8')\n",
    "movies = pd.read_csv('movies.csv', encoding='utf-8')\n",
    "genome_scores = pd.read_csv('genome-scores.csv', encoding='utf-8')\n",
    "genome_tags = pd.read_csv('genome-tags.csv', encoding='utf-8')\n",
    "\n",
    "print(f\"Ratings data: {ratings.shape}\\n\", ratings.head())\n",
    "print(f\"Movies data: {movies.shape}\\n\", movies.head())\n",
    "print(f\"Genome scores data: {genome_scores.shape}\\n\", genome_scores.head())\n",
    "print(f\"Genome tags data: {genome_tags.shape}\\n\", genome_tags.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge movies and ratings\n",
    "data = pd.merge(ratings, movies, on='movieId')\n",
    "\n",
    "# Create user-movie rating matrix\n",
    "rating_matrix = data.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "\n",
    "# Get number of users and movies\n",
    "n_users = data.userId.nunique()\n",
    "n_movies = data.movieId.nunique()\n",
    "\n",
    "# Create movie-tag relevance matrix\n",
    "movie_tag_matrix = genome_scores.pivot(index='movieId', columns='tagId', values='relevance').fillna(0)\n",
    "\n",
    "# Combine user-movie rating matrix with movie-tag relevance matrix\n",
    "combined_matrix = pd.concat([rating_matrix, movie_tag_matrix], axis=1, join='inner').fillna(0)\n",
    "\n",
    "# Convert to numpy array\n",
    "training_data = combined_matrix.values\n",
    "\n",
    "# Update number of visible units\n",
    "n_visible = training_data.shape[1]\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = train_test_split(data_matrix, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 500 # Number of hidden units\n",
    "\n",
    "# Placeholder for input data\n",
    "X = tf.placeholder(tf.float32, [None, n_visible])\n",
    "\n",
    "# Weights and biases\n",
    "W = tf.Variable(tf.random_normal([n_visible, n_hidden], mean=0.0, stddev=0.01))\n",
    "b_visible = tf.Variable(tf.zeros([n_visible]))\n",
    "b_hidden = tf.Variable(tf.zeros([n_hidden]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Core Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sample_hidden(X):\n",
    "    return tf.nn.sigmoid(tf.matmul(X, W) + b_hidden)\n",
    "\n",
    "def sample_visible(H):\n",
    "    return tf.nn.sigmoid(tf.matmul(H, tf.transpose(W)) + b_visible)\n",
    "\n",
    "# Gibbs sampling\n",
    "def gibbs_step(sample_k):\n",
    "    h_sample = sample_hidden(sample_k)\n",
    "    v_sample = sample_visible(h_sample)\n",
    "    h_sample = sample_hidden(v_sample)\n",
    "    return v_sample, h_sample\n",
    "\n",
    "# Contrastive Divergence algorithm\n",
    "def CD_k(X, k=1):\n",
    "    sample = X\n",
    "    for i in range(k):\n",
    "        v_sample, h_sample = gibbs_step(sample)\n",
    "        sample = v_sample\n",
    "    return X, h_sample, v_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric Collection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_ratings, predicted_ratings, k=10):\n",
    "    # Flatten arrays\n",
    "    true_ratings_flat = true_ratings.flatten()\n",
    "    predicted_ratings_flat = predicted_ratings.flatten()\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings_flat, predicted_ratings_flat))\n",
    "    mae = mean_absolute_error(true_ratings_flat, predicted_ratings_flat)\n",
    "\n",
    "    # Precision@k, Recall@k, and NDCG are more complex in implementation\n",
    "    # Here we simplify by using precision and recall at different thresholds\n",
    "    \n",
    "    # Compute Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(true_ratings_flat, predicted_ratings_flat)\n",
    "    \n",
    "    # Compute average precision score\n",
    "    map_score = average_precision_score(true_ratings_flat, predicted_ratings_flat)\n",
    "\n",
    "    return rmse, mae, precision, recall, map_score\n",
    "\n",
    "def log_metrics_to_wandb(true_ratings, predicted_ratings, epoch):\n",
    "    rmse, mae, precision, recall, map_score = compute_metrics(true_ratings, predicted_ratings)\n",
    "\n",
    "    # Log metrics\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MAP\": map_score\n",
    "    })\n",
    "    \n",
    "    # Log precision-recall curve\n",
    "    wandb.log({\n",
    "        \"precision-recall\": wandb.plot.precision_recall(y_true=true_ratings.flatten(), y_probas=predicted_ratings.flatten(), labels=None)\n",
    "    })\n",
    "\n",
    "    # Log histogram of predicted ratings\n",
    "    wandb.log({\n",
    "        \"predicted_ratings_histogram\": wandb.Histogram(predicted_ratings.flatten())\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialziing and Running Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for reconstruction\n",
    "X_sample, h_sample, v_sample = CD_k(X)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_mean(tf.square(X - v_sample))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=wandb.config.learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# Training\n",
    "n_epochs = wandb.config.epochs\n",
    "batch_size = wandb.config.batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0\n",
    "        total_batches = len(train_data) // batch_size\n",
    "        for i in range(total_batches):\n",
    "            batch = train_data[i * batch_size:(i + 1) * batch_size]\n",
    "            _, l = sess.run([train_op, loss], feed_dict={X: batch})\n",
    "            avg_loss += l / total_batches\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "        wandb.log({\"training_loss\": avg_loss})\n",
    "    \n",
    "    # Get the trained weights\n",
    "    trained_weights = sess.run(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating training reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "\n",
    "# Evaluate on test set\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(W.assign(trained_weights))\n",
    "    predicted_ratings = sess.run(gibbs_step(test_data))[0]\n",
    "\n",
    "# Log metrics for the test set\n",
    "log_metrics_to_wandb(test_data, predicted_ratings, epoch=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get movie recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_recommendations(new_user_movies, top_n=10):\n",
    "    user_vector = np.zeros((1, n_visible))\n",
    "\n",
    "    for movie_title, rating in new_user_movies:\n",
    "        # Find the movie ID for the given title\n",
    "        movie_id = movies[movies['title'] == movie_title]['movieId'].values[0]\n",
    "        user_vector[0, movie_id] = rating\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(W.assign(trained_weights))\n",
    "        v_sample, _ = sess.run(gibbs_step(user_vector))\n",
    "    \n",
    "    # Get the top N movie recommendations\n",
    "    recommendations = np.argsort(v_sample[0])[::-1][:top_n]\n",
    "    \n",
    "    # Filter out movies the user has already rated\n",
    "    rated_movie_ids = [movies[movies['title'] == title]['movieId'].values[0] for title, _ in new_user_movies]\n",
    "    recommendations = [rec for rec in recommendations if rec not in rated_movie_ids]\n",
    "    \n",
    "    recommended_movies = movies[movies['movieId'].isin(recommendations)]\n",
    "    \n",
    "    return recommended_movies['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "new_user_movies = [(\"Toy Story (1995)\", 5.0), (\"Jumanji (1995)\", 4.0)]\n",
    "print(get_movie_recommendations(new_user_movies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
